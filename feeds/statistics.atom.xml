<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Lisha Li's Blog</title><link href="http://lisafly.github.io/" rel="alternate"></link><link href="http://lisafly.github.io/feeds/statistics.atom.xml" rel="self"></link><id>http://lisafly.github.io/</id><updated>2013-03-22T00:00:00-07:00</updated><entry><title>Aggregation</title><link href="http://lisafly.github.io/blog/aggregation/" rel="alternate"></link><updated>2013-03-22T00:00:00-07:00</updated><author><name>Chuanlong (Ben) Du</name></author><id>tag:lisafly.github.io,2013-03-22:blog/aggregation/</id><summary type="html">&lt;p&gt;The prediction error is a trade-off of bias and variance. 
In statistics, 
we often talk about unbiased estimators (especially in linear regression). 
In this case we restrict the estimators/predictors to be in a (small) class,
and find the optimal solution in this class (called BLUE or BLUP).&lt;/p&gt;
&lt;p&gt;Generally speaking, unbiased predictors are not ideas in prediction problems.
In many situations we can easily find predictors with small bias (usually very complexed predictors/models). 
To improve the performance of predictors, 
we often want to decrease their variance at the cost of increase bias. 
This is called decreasing the complexity of the predictors/models.
This can be done by variable/feature selection. 
This is discussed in &lt;a href=""&gt;the regularization post&lt;/a&gt;.
Here we talk about some aggregation/ensamble technic to decrease the complexity of predictors/models.
The is inspired by the simple fact that the variance of the mean of &lt;code&gt;n&lt;/code&gt; iid random variables (with finite variance)
is &lt;code&gt;1/n&lt;/code&gt; times the variance of any of them. 
This means that by combining (not strongly correlated) predictors,
e.g., linear combination or majority vote, 
we can get (much) better predictors. &lt;/p&gt;
&lt;h2&gt;Decrease Complexity&lt;/h2&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Bagging is short for Bootstrap Aggregation.
The basic idea is to build predictors based on bootstrap samples (sample with replacement) of the training data. 
Each time we get a different bootstrap sample,
so we have a different predictor.
Hopefully these predictors are not strongly correlated so that by averaging (unweighted) them we get a better predictor.
This idea is well used in random forest. 
To further decrease the correlation between trees, 
the random forest process also restricts the number of variables 
(usually square root of the total number of variables) used to build each tree.
The neuron network can also be considered as a way of Bagging,
since the input of a node is a linear combination of outcomes from last layer.&lt;/p&gt;
&lt;h3&gt;Stacking&lt;/h3&gt;
&lt;p&gt;Stacking is similar to Bagging. 
The difference is that Stacking uses weighted averages of predictors based on their performances. 
The weights are often chosen to minized the prediction error in leave-1-out cross validation. &lt;/p&gt;
&lt;h2&gt;Increase Complexity&lt;/h2&gt;
&lt;p&gt;An opposite approach to these discussed above is to start from a very simple predictor (big bias but small variance)
and then increase the complexity (decrease bias at cost of increase variance) of the predictor.
This is common in nature, e.g., the human brain is developped from simple to complex. 
This approach is called Boosting.
There are different version of Boosting, e.g., Gradient Boosting and AdaBoosting (Adaptive Boosting).
AdaBoosting is popular in face recognition problems.&lt;/p&gt;</summary><category term="statistics"></category><category term="ensamble"></category><category term="aggregation"></category><category term="data mining"></category><category term="machine learning"></category></entry><entry><title>Summary on Random Number Generators</title><link href="http://lisafly.github.io/blog/summary-random-number-generators/" rel="alternate"></link><updated>2012-07-24T00:00:00-07:00</updated><author><name>Chuanlong (Ben) Du</name></author><id>tag:lisafly.github.io,2012-07-24:blog/summary-random-number-generators/</id><summary type="html">&lt;p&gt;&lt;img src="http://dclong.github.io/media/rng/random-number-generator.png" height="200" width="240" align="right"/&gt;&lt;/p&gt;
&lt;p&gt;The most popular pseudo random number generator (PRNG) currently is Mersenne Twister. 
Mersenne Twister has many different versions, among which the MT19937 
is the most widely used one. 
The period of MT19937 is extreemly long ($2^{19937}-1$)
and is equidistributed for generating vectors up to dimension 623. 
The MT19937 generate 32 bits random numbers. 
Combining two random blocks, one can generate 64 bits random numbers.
This is often implemented together with the 32 bit version, 
and usually call MT19937_64.
The MT19937_64 is equidistributed for generating vectors up to dimension 311.
A more modern family of random number generators than Mersenne Twister 
is the WELL random number generators, which have better equidistribution 
property and are better to escape the zeroland (initialization 
array contains many zero bits). 
However, the speed of the WELL generators is about 0.6 to 0.7 compared to the Mersenne Twister generators. 
Also the WELL random number generators has a large inner state 
(e.g., the WELL44497b uses about 33kb for its inner state while the MT19937 uses only about 2.5kb). 
This is usually not a problem on modern computers, 
but if you use lots of random number generators at the same time or if the code is run on a embedded device, 
it might worth considering the consume of memories of these generators. 
Among different versions of WELL generators, WELL19937c and WELL44497b are commonly used. 
SIMD-oriented Fast Mersenne Twister is an improved version of Mersenne Twister. 
It uses parallelism of modern CPUs and is about twice faster than mersenne Twister. 
SFMT also has better equidistribution property than Mersenne Twister, but not as good as WELL.
SFMT recovers from 0-excess initial state faster than Mersenne Twister, but not faster than WELL.
It is likely that SFMT replaces Mersenne Twister and becomes the next popular number generator. &lt;/p&gt;</summary><category term="WELL"></category><category term="RNG"></category><category term="statistics"></category><category term="SFMT"></category><category term="dimension"></category><category term="MT"></category></entry></feed>